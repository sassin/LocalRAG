# Architecture Overview

LocalRAG follows a strict pipeline:

Retrieval â†’ Reasoning â†’ Memory

## High-level flow

Documents â†’ Indexer â†’ Local Vector Store â†’ Retrieval Tools â†’ LLM â†’ User

## Key principles

1. Retrieval is mandatory for factual answers
2. Memory never replaces retrieval
3. Documents never leave the local machine
4. The LLM only sees:
   - The user question
   - Retrieved excerpts
   - A small session context

## Core components

### Indexer (`rag/index.py`)
- Extracts text from documents
- Chunks content
- Generates embeddings
- Stores vectors locally (FAISS)

### Vector Store (`rag/store.py`)
- FAISS-backed
- Cosine similarity search
- No remote calls

### Retrieval Tools (`rag/tool.py`)
- `rag_search_2pass`: default retrieval
- `rag_get_page`: page-specific retrieval

### Prompt Layer (`prompts/`)
- Shared across CLI + Web
- Enforces evidence grounding

### Session Memory (`memory.py`)
- Lightweight, bounded memory
- Keeps conversational continuity

### Interfaces
- CLI (ADK agent)
- Web UI (FastAPI)


                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Documents  â”‚
                â”‚ (PDF, TXTâ€¦)  â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Indexer     â”‚  â† index.py
                â”‚  (FAISS)     â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Vector DB   â”‚  â† LocalRAGStore
                â”‚ (local)      â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLI (ADK)    â”‚              â”‚ FastAPI Web  â”‚
â”‚ chat_cli     â”‚              â”‚ server.py    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                              â”‚
       â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ResearchAgentâ”‚              â”‚ Prompt Builderâ”‚
â”‚ (with tools) â”‚              â”‚ + Memory      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LLM (OpenAI / Gemini)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



ğŸ“„ Supported Document Types
Indexing (default):
.pdf (text-based)
.txt
.csv
.xlsx
.docx
âš ï¸ Complex PDFs with scanned pages or multi-column tables should be handled separately (OCR / Docling pipeline can be added later).